{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6e8d20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Add project root to path\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m project_root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(__file__).parent.parent if __name__ == \"__main__\" else Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import config\n",
    "import utils\n",
    "from scripts.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Load and clean data\n",
    "train_df, test_df = utils.load_data()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Train / Val Split\n",
    "X = train_df.drop(columns=['trip_id', 'travel_time'])\n",
    "y = train_df['travel_time']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# RMSE helper function\n",
    "rmse = lambda true, pred: np.sqrt(mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/1d589ljx3q314l0v26w1jpsm0000gn/T/ipykernel_5228/66368720.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  weather_df['weather_hour'] = weather_df['lcl_datetime'].dt.floor('H')\n",
      "/var/folders/x5/1d589ljx3q314l0v26w1jpsm0000gn/T/ipykernel_5228/66368720.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  test_df['trip_hour'] = test_df['lcl_start_transporting_dttm'].dt.floor('H')\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m test_enc = preprocessor.transform(test_df)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Make predictions using the XGBoost model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m test_predictions = \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Create submission DataFrame\u001b[39;00m\n\u001b[32m     67\u001b[39m submission = pd.DataFrame({\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrip_id\u001b[39m\u001b[33m\"\u001b[39m: trip_ids,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTarget\u001b[39m\u001b[33m\"\u001b[39m: test_predictions\n\u001b[32m     70\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.inplace_predict(\n\u001b[32m   1328\u001b[39m             data=X,\n\u001b[32m   1329\u001b[39m             iteration_range=iteration_range,\n\u001b[32m   1330\u001b[39m             predict_type=\u001b[33m\"\u001b[39m\u001b[33mmargin\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_margin \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1331\u001b[39m             missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1332\u001b[39m             base_margin=base_margin,\n\u001b[32m   1333\u001b[39m             validate_features=validate_features,\n\u001b[32m   1334\u001b[39m         )\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:922\u001b[39m, in \u001b[36mXGBModel.get_booster\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__sklearn_is_fitted__():\n\u001b[32m    920\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[33m\"\u001b[39m\u001b[33mneed to call fit or load_model beforehand\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._Booster\n",
      "\u001b[31mNotFittedError\u001b[39m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "    \"# Generate submission file\n",
    "\",\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"../data/raw/Test.csv\")\n",
    "\n",
    "# Store trip_ids first before any preprocessing\n",
    "trip_ids = test_df['trip_id'].copy()\n",
    "\n",
    "# Load weather data for test set\n",
    "weather_df = pd.read_csv(\"../data/raw/Accra_weather.csv\")\n",
    "weather_df['lcl_datetime'] = pd.to_datetime(weather_df['lcl_datetime'])\n",
    "weather_df['weather_hour'] = weather_df['lcl_datetime'].dt.floor('H')\n",
    "\n",
    "# Convert datetime columns in test data\n",
    "test_df['lcl_start_transporting_dttm'] = pd.to_datetime(test_df['lcl_start_transporting_dt'])\n",
    "test_df['trip_hour'] = test_df['lcl_start_transporting_dttm'].dt.floor('H')\n",
    "\n",
    "# Extract time features\n",
    "test_df['hour'] = test_df['lcl_start_transporting_dttm'].dt.hour\n",
    "test_df['day_of_week'] = test_df['lcl_start_transporting_dttm'].dt.dayofweek\n",
    "test_df['day_of_month'] = test_df['lcl_start_transporting_dttm'].dt.day\n",
    "test_df['is_weekend'] = (test_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Define rush hours\n",
    "def get_rush_hour(hour):\n",
    "    if 7 <= hour <= 9 or 17 <= hour <= 19:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "test_df['is_rush_hour'] = test_df['hour'].apply(get_rush_hour)\n",
    "\n",
    "# Calculate haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Earth radius in kilometers\n",
    "    return c * r\n",
    "\n",
    "test_df['haversine_distance_km'] = test_df.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['origin_lat'], row['origin_lon'], \n",
    "        row['destination_lat'], row['destination_lon']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Merge with weather data\n",
    "test_df = test_df.merge(\n",
    "    weather_df[['weather_hour', 'precipitation_type', 'prev_hour_precipitation_mm', 'temperature_C']],\n",
    "    left_on='trip_hour',\n",
    "    right_on='weather_hour',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns and ensure same structure as training data\n",
    "test_df = test_df.drop(columns=[c for c in DROP_COLS if c in test_df.columns], errors=\"ignore\")\n",
    "\n",
    "# Preprocess test data using the same preprocessor\n",
    "test_enc = preprocessor.transform(test_df)\n",
    "\n",
    "# Make predictions using the XGBoost model\n",
    "test_predictions = xgb_model.predict(test_enc)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"trip_id\": trip_ids,\n",
    "    \"Target\": test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"../outputs/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b6b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest       RMSE: 4.4665\n",
      "Linear Regression   RMSE: 12.2044\n",
      "Linear Regression   RMSE: 12.2044\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     87\u001b[39m lgb_val   = lgb.Dataset(X_val_enc,   label=y_val, reference=lgb_train)\n\u001b[32m     89\u001b[39m lgb_params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     90\u001b[39m     objective      = \u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m     metric         = \u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m     random_state   = RANDOM_SEED\n\u001b[32m     97\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m lgb_model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlgb_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    106\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m lgb_rmse = rmse(y_val, lgb_model.predict(X_val_enc))\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLightGBM (CPU)     RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgb_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: train() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# # ==============================================================\n",
    "# #  Yango Accra Mobility – Step 2: Model Training (Apple‑Silicon)\n",
    "# # ==============================================================\n",
    "\n",
    "# import pandas as pd, numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # ----------------- Config ----------------------------------------------------\n",
    "# RANDOM_SEED   = 42\n",
    "# TARGET        = \"Target\"\n",
    "# FILE_PATH     = \"../data/raw/train_with_weather.csv\"\n",
    "# DROP_COLS     = [\"trip_id\", \"ID\"]             # drop if present\n",
    "# TEST_SIZE     = 0.20\n",
    "\n",
    "# # ----------------- 1. Load Dataset ------------------------------------------\n",
    "# df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# # ----------------- 2. Separate Target & Drop IDs ----------------------------\n",
    "# y = df[TARGET]\n",
    "# X = df.drop(columns=[TARGET] + [c for c in DROP_COLS if c in df.columns],\n",
    "#             errors=\"ignore\")\n",
    "\n",
    "# # ----------------- 3. Column Groups -----------------------------------------\n",
    "# cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "# num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# # ----------------- 4. Preprocessor (Impute + Ordinal Encode) -----------------\n",
    "# cat_pipe = make_pipeline(\n",
    "#     SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "#     OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "# )\n",
    "# num_pipe = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# pre = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", num_pipe, num_cols),\n",
    "#         (\"cat\", cat_pipe, cat_cols)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # ----------------- 5. Train / Validation Split ------------------------------\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    "# )\n",
    "\n",
    "# rmse = lambda t, p: np.sqrt(mean_squared_error(t, p))\n",
    "\n",
    "# # ----------------- 6. Random Forest -----------------------------------------\n",
    "# rf_pipe = Pipeline([\n",
    "#     (\"pre\", pre),\n",
    "#     (\"model\", RandomForestRegressor(\n",
    "#         n_estimators=300,\n",
    "#         n_jobs=-1,                 # use all CPU cores on M‑series\n",
    "#         random_state=RANDOM_SEED))\n",
    "# ])\n",
    "# rf_pipe.fit(X_train, y_train)\n",
    "# rf_rmse = rmse(y_val, rf_pipe.predict(X_val))\n",
    "# print(f\"Random Forest       RMSE: {rf_rmse:.4f}\")\n",
    "\n",
    "# # ----------------- 7. Linear Regression -------------------------------------\n",
    "# lr_pipe = Pipeline([(\"pre\", pre), (\"model\", LinearRegression())])\n",
    "# lr_pipe.fit(X_train, y_train)\n",
    "# lr_rmse = rmse(y_val, lr_pipe.predict(X_val))\n",
    "# print(f\"Linear Regression   RMSE: {lr_rmse:.4f}\")\n",
    "\n",
    "# # ----------------- 8. Prepare Encoded Matrices for Boosters ------------------\n",
    "# X_train_enc = pre.fit_transform(X_train)\n",
    "# X_val_enc   = pre.transform(X_val)\n",
    "\n",
    "# # keep sparse matrices sparse for speed / memory\n",
    "# if hasattr(X_train_enc, \"toarray\"):  # (it’s sparse if OneHot used; safe check)\n",
    "#     X_train_enc = X_train_enc\n",
    "#     X_val_enc   = X_val_enc\n",
    "\n",
    "# # ----------------- 9. LightGBM (CPU) ----------------------------------------\n",
    "# lgb_train = lgb.Dataset(X_train_enc, label=y_train)\n",
    "# lgb_val   = lgb.Dataset(X_val_enc,   label=y_val, reference=lgb_train)\n",
    "\n",
    "# lgb_params = dict(\n",
    "#     objective      = \"regression\",\n",
    "#     metric         = \"rmse\",\n",
    "#     boosting_type  = \"gbdt\",\n",
    "#     num_leaves     = 31,\n",
    "#     learning_rate  = 0.05,\n",
    "#     num_threads    = 0,        # 0 = all CPU cores\n",
    "#     random_state   = RANDOM_SEED\n",
    "# )\n",
    "\n",
    "# lgb_model = lgb.train(\n",
    "#     lgb_params,\n",
    "#     lgb_train,\n",
    "#     valid_sets        = [lgb_val],\n",
    "#     num_boost_round   = 500,\n",
    "#     early_stopping_rounds = 20,\n",
    "#     verbose_eval      = False\n",
    "# )\n",
    "# lgb_rmse = rmse(y_val, lgb_model.predict(X_val_enc))\n",
    "# print(f\"LightGBM (CPU)     RMSE: {lgb_rmse:.4f}\")\n",
    "\n",
    "# # ----------------- 10. XGBoost (CPU‑hist) ------------------------------------\n",
    "# xgb_model = xgb.XGBRegressor(\n",
    "#     objective          = \"reg:squarederror\",\n",
    "#     eval_metric        = \"rmse\",\n",
    "#     tree_method        = \"hist\",      # fastest CPU algorithm\n",
    "#     learning_rate      = 0.05,\n",
    "#     n_estimators       = 1000,\n",
    "#     max_depth          = 8,\n",
    "#     enable_categorical = True,        # works with ordinal ints\n",
    "#     n_jobs             = 0,           # all cores\n",
    "#     random_state       = RANDOM_SEED\n",
    "# )\n",
    "# xgb_model.fit(\n",
    "#     X_train_enc, y_train,\n",
    "#     eval_set=[(X_val_enc, y_val)],\n",
    "#     early_stopping_rounds=30,\n",
    "#     verbose=False\n",
    "# )\n",
    "# xgb_rmse = rmse(y_val, xgb_model.predict(X_val_enc))\n",
    "# print(f\"XGBoost (CPU)      RMSE: {xgb_rmse:.4f}\")\n",
    "\n",
    "# # ----------------- 11. Summary ----------------------------------------------\n",
    "# rmse_scores = {\n",
    "#     \"Random Forest\"     : rf_rmse,\n",
    "#     \"Linear Regression\" : lr_rmse,\n",
    "#     \"LightGBM (CPU)\"    : lgb_rmse,\n",
    "#     \"XGBoost (CPU)\"     : xgb_rmse\n",
    "# }\n",
    "# best_model = min(rmse_scores, key=rmse_scores.get)\n",
    "# print(\"\\nRMSE summary:\", {k: f\"{v:.4f}\" for k, v in rmse_scores.items()})\n",
    "# print(f\"Best model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55a7a4",
   "metadata": {},
   "source": [
    "<!-- # Model Training Notebook\n",
    "\n",
    "This notebook is dedicated to training machine learning models for the Yango Accra Mobility Prediction Hackathon. The goal is to predict ride times using trip and weather data.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Import required libraries\n",
    "2. Load and preprocess data\n",
    "3. Train-test split\n",
    "4. Train baseline models\n",
    "5. Train advanced models (e.g., LightGBM, XGBoost)\n",
    "6. Evaluate models using RMSE\n",
    "7. Save the best model -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21745812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# # Import required libraries\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import joblib\n",
    "\n",
    "# print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset shape: (57596, 21)\n",
      "         trip_id  destination_lat  destination_lon lcl_start_transporting_dt  \\\n",
      "0  ID_S3BD1V9G53         5.630927        -0.169211                2024-05-05   \n",
      "1  ID_ZJM7LMN65Q         5.645044        -0.156482                2024-05-21   \n",
      "2  ID_SZ3BP6V01V         5.711156        -0.141063                2024-05-05   \n",
      "3  ID_5IPHXDCMKF         5.677497        -0.183350                2024-05-26   \n",
      "4  ID_BYZEJ0B5RA         5.601700        -0.173589                2024-05-30   \n",
      "\n",
      "  lcl_start_transporting_dttm  origin_lat  origin_lon  str_distance_km  \\\n",
      "0         2024-05-05 09:56:32    5.630979   -0.164760            0.529   \n",
      "1         2024-05-21 10:53:32    5.686892   -0.118931            6.230   \n",
      "2         2024-05-05 21:21:21    5.706008   -0.164999            2.705   \n",
      "3         2024-05-26 21:23:33    5.665943   -0.182602            1.236   \n",
      "4         2024-05-30 14:02:13    5.565401   -0.160919            4.312   \n",
      "\n",
      "   transporting_distance_fact_km  Target  ...  day_of_week  day_of_month  \\\n",
      "0                          0.850    2.18  ...            6             5   \n",
      "1                          8.720   20.93  ...            1            21   \n",
      "2                          3.239   13.02  ...            6             5   \n",
      "3                          1.410    3.80  ...            6            26   \n",
      "4                          6.553   17.23  ...            3            30   \n",
      "\n",
      "   is_weekend  is_rush_hour  haversine_distance_km            trip_hour  \\\n",
      "0           1             1               0.492566  2024-05-05 09:00:00   \n",
      "1           0             0               6.238359  2024-05-21 10:00:00   \n",
      "2           1             0               2.709589  2024-05-05 21:00:00   \n",
      "3           1             0               1.287398  2024-05-26 21:00:00   \n",
      "4           0             0               4.272852  2024-05-30 14:00:00   \n",
      "\n",
      "          weather_hour precipitation_type prev_hour_precipitation_mm  \\\n",
      "0  2024-05-05 09:00:00               Rain                   0.012875   \n",
      "1  2024-05-21 10:00:00               Rain                   0.001907   \n",
      "2  2024-05-05 21:00:00               Rain                   0.001907   \n",
      "3  2024-05-26 21:00:00   No precipitation                   0.000000   \n",
      "4  2024-05-30 14:00:00   No precipitation                   0.019073   \n",
      "\n",
      "   temperature_C  \n",
      "0          28.74  \n",
      "1          29.32  \n",
      "2          29.54  \n",
      "3          28.49  \n",
      "4          29.38  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "best_model = min(rmse_scores, key=rmse_scores.get)\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best RMSE: {rmse_scores[best_model]:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model_obj = None\n",
    "if best_model == \"Random Forest\":\n",
    "    best_model_obj = rf_model\n",
    "elif best_model == \"Linear Regression\": \n",
    "    best_model_obj = lr_model\n",
    "elif best_model == \"LightGBM (CPU)\":\n",
    "    best_model_obj = lgb_model\n",
    "elif best_model == \"XGBoost (CPU)\":\n",
    "    best_model_obj = xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83816a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed!\n",
      "Features: ['destination_lat', 'destination_lon', 'origin_lat', 'origin_lon', 'str_distance_km', 'transporting_distance_fact_km', 'hour', 'day_of_week', 'day_of_month', 'is_weekend', 'is_rush_hour', 'haversine_distance_km', 'prev_hour_precipitation_mm', 'temperature_C']\n"
     ]
    }
   ],
   "source": [
    "# # Preprocess data\n",
    "# # Ensure all features are numeric and handle missing values\n",
    "# data = data.select_dtypes(include=[np.number]).dropna()\n",
    "\n",
    "# # Define target and features\n",
    "# target = 'Target'\n",
    "# features = [col for col in data.columns if col != target]\n",
    "\n",
    "# X = data[features]\n",
    "# y = data[target]\n",
    "\n",
    "# print(\"Data preprocessing completed!\")\n",
    "# print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed!\n",
      "Training set size: 46071 samples\n",
      "Test set size: 11518 samples\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "test_predictions = best_model_obj.predict(X_test_enc)\n",
    "submission = pd.DataFrame({\n",
    "    'trip_id': test_df['trip_id'],\n",
    "    'travel_time': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = config.PROJECT_ROOT / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f05255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 5.63\n",
      "\n",
      "Random Forest RMSE: 6.07\n",
      "Random Forest RMSE: 6.07\n"
     ]
    }
   ],
   "source": [
    "# # Train baseline models\n",
    "# # Linear Regression\n",
    "# lr_model = LinearRegression()\n",
    "# lr_model.fit(X_train, y_train)\n",
    "# lr_predictions = lr_model.predict(X_test)\n",
    "# lr_mse = mean_squared_error(y_test, lr_predictions)\n",
    "# lr_rmse = np.sqrt(lr_mse)\n",
    "\n",
    "# print(f\"Linear Regression RMSE: {lr_rmse:.2f}\")\n",
    "\n",
    "# # Random Fores\\t\n",
    "# rf_model = RandomForestRegressor(random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# rf_mse = mean_squared_error(y_test, rf_predictions)\n",
    "# rf_rmse = np.sqrt(rf_mse)\n",
    "\n",
    "# print(f\"Random Forest RMSE: {rf_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3525e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: lcl_start_transporting_dt: object, lcl_start_transporting_dttm: object, trip_hour: object, weather_hour: object, precipitation_type: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Train LightGBM model\u001b[39;00m\n\u001b[32m     17\u001b[39m evals_result = {}\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m lgb_model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlgb_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m lgb_predictions = lgb_model.predict(X_test)\n\u001b[32m     30\u001b[39m lgb_mse = mean_squared_error(y_test, lgb_predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/engine.py:297\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     booster = \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[32m    299\u001b[39m         booster.set_train_data_name(train_data_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:3656\u001b[39m, in \u001b[36mBooster.__init__\u001b[39m\u001b[34m(self, params, train_set, model_file, model_str)\u001b[39m\n\u001b[32m   3649\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_network(\n\u001b[32m   3650\u001b[39m         machines=machines,\n\u001b[32m   3651\u001b[39m         local_listen_port=params[\u001b[33m\"\u001b[39m\u001b[33mlocal_listen_port\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3652\u001b[39m         listen_time_out=params.get(\u001b[33m\"\u001b[39m\u001b[33mtime_out\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m120\u001b[39m),\n\u001b[32m   3653\u001b[39m         num_machines=params[\u001b[33m\"\u001b[39m\u001b[33mnum_machines\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3654\u001b[39m     )\n\u001b[32m   3655\u001b[39m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3656\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3657\u001b[39m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[32m   3658\u001b[39m params.update(train_set.get_params())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:2590\u001b[39m, in \u001b[36mDataset.construct\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2585\u001b[39m             \u001b[38;5;28mself\u001b[39m._set_init_score_by_predictor(\n\u001b[32m   2586\u001b[39m                 predictor=\u001b[38;5;28mself\u001b[39m._predictor, data=\u001b[38;5;28mself\u001b[39m.data, used_indices=used_indices\n\u001b[32m   2587\u001b[39m             )\n\u001b[32m   2588\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2590\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.free_raw_data:\n\u001b[32m   2604\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:2123\u001b[39m, in \u001b[36mDataset._lazy_init\u001b[39m\u001b[34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[39m\n\u001b[32m   2121\u001b[39m     categorical_feature = reference.categorical_feature\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m.pandas_categorical = \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2130\u001b[39m     feature_name = data.column_names\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:868\u001b[39m, in \u001b[36m_data_from_pandas\u001b[39m\u001b[34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[39m\n\u001b[32m    864\u001b[39m df_dtypes.append(np.float32)\n\u001b[32m    865\u001b[39m target_dtype = np.result_type(*df_dtypes)\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    869\u001b[39m     feature_name,\n\u001b[32m    870\u001b[39m     categorical_feature,\n\u001b[32m    871\u001b[39m     pandas_categorical,\n\u001b[32m    872\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:814\u001b[39m, in \u001b[36m_pandas_to_numpy\u001b[39m\u001b[34m(data, target_dtype)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pandas_to_numpy\u001b[39m(\n\u001b[32m    811\u001b[39m     data: pd_DataFrame,\n\u001b[32m    812\u001b[39m     target_dtype: \u001b[33m\"\u001b[39m\u001b[33mnp.typing.DTypeLike\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    813\u001b[39m ) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    816\u001b[39m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[32m    817\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data.to_numpy(dtype=target_dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yango Accra Mobility Prediction Hackathon/.venv/lib/python3.12/site-packages/lightgbm/basic.py:805\u001b[39m, in \u001b[36m_check_for_bad_pandas_dtypes\u001b[39m\u001b[34m(pandas_dtypes_series)\u001b[39m\n\u001b[32m    799\u001b[39m bad_pandas_dtypes = [\n\u001b[32m    800\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series.items()\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype.type)\n\u001b[32m    803\u001b[39m ]\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    806\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: lcl_start_transporting_dt: object, lcl_start_transporting_dttm: object, trip_hour: object, weather_hour: object, precipitation_type: object"
     ]
    }
   ],
   "source": [
    "# # # Train advanced models\n",
    "# # # LightGBM\n",
    "# lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "# lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "# lgb_params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'learning_rate': 0.1,\n",
    "#     'num_leaves': 31,\n",
    "#     'random_state': 42,\n",
    "#     'verbosity': -1\n",
    "# }\n",
    "\n",
    "# # Train LightGBM model\n",
    "# evals_result = {}\n",
    "# lgb_model = lgb.train(\n",
    "#     params=lgb_params,\n",
    "#     train_set=lgb_train,\n",
    "#     valid_sets=[lgb_train, lgb_test],\n",
    "#     num_boost_round=100,\n",
    "#     callbacks=[\n",
    "#         lgb.callback.early_stopping(stopping_rounds=20),\n",
    "#         lgb.callback.record_evaluation(evals_result)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# lgb_predictions = lgb_model.predict(X_test)\n",
    "# lgb_mse = mean_squared_error(y_test, lgb_predictions)\n",
    "# lgb_rmse = np.sqrt(lgb_mse)\n",
    "\n",
    "# print(f\"LightGBM RMSE: {lgb_rmse:.2f}\")\n",
    "\n",
    "# # # XGBoost\n",
    "# xgb_model = xgb.XGBRegressor(\n",
    "#     objective='reg:squarederror',\n",
    "#     random_state=42,\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1\n",
    "# )\n",
    "# xgb_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     eval_set=[(X_test, y_test)],\n",
    "#     early_stopping_rounds=20,\n",
    "#     verbose=False\n",
    "# )\n",
    "# xgb_predictions = xgb_model.predict(X_test)\n",
    "# xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "# xgb_rmse = np.sqrt(xgb_mse)\n",
    "\n",
    "# print(f\"XGBoost RMSE: {xgb_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = min([(lr_model, lr_rmse), (rf_model, rf_rmse), (lgb_model, lgb_rmse), (xgb_model, xgb_rmse)], key=lambda x: x[1])[0]\n",
    "# joblib.dump(best_model, '../outputs/best_model.pkl')\n",
    "\n",
    "# print(\"Best model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'trip_id': X_test.index,\n",
    "#     'Target': xgb_predictions\n",
    "# })\n",
    "\n",
    "# submission_file_path = '../outputs/submission.csv'\n",
    "# submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "# print(f\"Submission file saved to {submission_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
