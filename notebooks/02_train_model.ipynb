{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6e8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\CALYX BLAY\\OneDrive\\Desktop\\yango-accra-mobility-prediction\\data\\raw\n",
      "Train file path: c:\\Users\\CALYX BLAY\\OneDrive\\Desktop\\yango-accra-mobility-prediction\\data\\raw\\Train.csv\n",
      "File exists: True\n",
      "Data loaded successfully!\n",
      "Train: (57596, 10), Test: (24684, 9)\n",
      "Weather: (744, 5), Sample: (24684, 2)\n",
      "Data loaded successfully!\n",
      "Train: (57596, 10), Test: (24684, 9)\n",
      "Weather: (744, 5), Sample: (24684, 2)\n",
      "Data loaded successfully!\n",
      "Training features shape: (57596, 8)\n",
      "Target shape: (57596,)\n",
      "Feature columns: ['destination_lat', 'destination_lon', 'lcl_start_transporting_dt', 'lcl_start_transporting_dttm', 'origin_lat', 'origin_lon', 'str_distance_km', 'transporting_distance_fact_km']\n",
      "Data loaded successfully!\n",
      "Training features shape: (57596, 8)\n",
      "Target shape: (57596,)\n",
      "Feature columns: ['destination_lat', 'destination_lon', 'lcl_start_transporting_dt', 'lcl_start_transporting_dttm', 'origin_lat', 'origin_lon', 'str_distance_km', 'transporting_distance_fact_km']\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add project root to path (notebook-friendly approach)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import and reload modules to ensure fresh imports\n",
    "import config\n",
    "import utils\n",
    "from scripts.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Reload modules to pick up any changes\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Verify paths are correct\n",
    "print(f\"Data directory: {config.DATA_DIR}\")\n",
    "print(f\"Train file path: {config.TRAIN_FILE}\")\n",
    "print(f\"File exists: {config.TRAIN_FILE.exists()}\")\n",
    "\n",
    "# Load and clean data\n",
    "train_df, test_df, weather_df, sample_submission = utils.load_data()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Train / Val Split - using correct target column name\n",
    "X = train_df.drop(columns=['trip_id', 'Target'])  # Fixed: Target instead of travel_time\n",
    "y = train_df['Target']  # Fixed: Target instead of travel_time\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# RMSE helper function\n",
    "rmse = lambda true, pred: np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce88bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (46076, 8)\n",
      "Validation data shape: (11520, 8)\n",
      "Categorical columns: ['lcl_start_transporting_dt', 'lcl_start_transporting_dttm']\n",
      "Numerical columns: ['destination_lat', 'destination_lon', 'origin_lat', 'origin_lon', 'str_distance_km', 'transporting_distance_fact_km']\n"
     ]
    }
   ],
   "source": [
    "# Column groups for preprocessing\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "num_pipe = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encode training and validation data\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_val_enc = preprocessor.transform(X_val)\n",
    "\n",
    "print(f\"Training data shape: {X_train_enc.shape}\")\n",
    "print(f\"Validation data shape: {X_val_enc.shape}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "print(f\"Numerical columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79528a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest       RMSE: 4.6361\n",
      "Linear Regression   RMSE: 19.2243\n",
      "Linear Regression   RMSE: 19.2243\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", rf_model)\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "rf_rmse = rmse(y_val, rf_pipe.predict(X_val))\n",
    "print(f\"Random Forest       RMSE: {rf_rmse:.4f}\")\n",
    "\n",
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_pipe = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", lr_model)\n",
    "])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_rmse = rmse(y_val, lr_pipe.predict(X_val))\n",
    "print(f\"Linear Regression   RMSE: {lr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5623d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1817\n",
      "[LightGBM] [Info] Number of data points in the train set: 46076, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 10.095750\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's rmse: 6.30884\n",
      "[20]\tvalid_0's rmse: 5.29908\n",
      "[30]\tvalid_0's rmse: 4.93001\n",
      "[40]\tvalid_0's rmse: 4.79448\n",
      "[50]\tvalid_0's rmse: 4.7308\n",
      "[30]\tvalid_0's rmse: 4.93001\n",
      "[40]\tvalid_0's rmse: 4.79448\n",
      "[50]\tvalid_0's rmse: 4.7308\n",
      "[60]\tvalid_0's rmse: 4.7382\n",
      "[70]\tvalid_0's rmse: 4.74863\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's rmse: 4.72548\n",
      "LightGBM (CPU)      RMSE: 4.7255\n",
      "[60]\tvalid_0's rmse: 4.7382\n",
      "[70]\tvalid_0's rmse: 4.74863\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's rmse: 4.72548\n",
      "LightGBM (CPU)      RMSE: 4.7255\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Model\n",
    "lgb_train = lgb.Dataset(X_train_enc, label=y_train)\n",
    "lgb_val = lgb.Dataset(X_val_enc, label=y_val, reference=lgb_train)\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"num_threads\": 0,\n",
    "    \"random_state\": RANDOM_SEED\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_set = lgb_train,\n",
    "    valid_sets = [lgb_val],\n",
    "    num_boost_round = 500,\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=10)\n",
    "    ]\n",
    ")\n",
    "lgb_rmse = rmse(y_val, lgb_model.predict(X_val_enc, num_iteration=lgb_model.best_iteration))\n",
    "print(f\"LightGBM (CPU)      RMSE: {lgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313c987f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\n\u001b[0;32m      3\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Fit XGBoost model with early stopping - using keyword arguments explicitly\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m xgb_rmse \u001b[38;5;241m=\u001b[39m rmse(y_val, xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_val_enc))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost (CPU)       RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CALYX BLAY\\OneDrive\\Desktop\\yango-accra-mobility-prediction\\.venv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",      # fastest CPU algorithm\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    n_jobs=0,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Fit XGBoost model with early stopping - using keyword arguments explicitly\n",
    "xgb_model.fit(\n",
    "    X=X_train_enc, \n",
    "    y=y_train,\n",
    "    eval_set=[(X_val_enc, y_val)],\n",
    "    early_stopping_rounds=30,\n",
    "    verbose=False\n",
    ")\n",
    "xgb_rmse = rmse(y_val, xgb_model.predict(X_val_enc))\n",
    "print(f\"XGBoost (CPU)       RMSE: {xgb_rmse:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "rmse_scores = {\n",
    "    \"Random Forest\": rf_rmse,\n",
    "    \"Linear Regression\": lr_rmse,\n",
    "    \"LightGBM (CPU)\": lgb_rmse,\n",
    "    \"XGBoost (CPU)\": xgb_rmse\n",
    "}\n",
    "\n",
    "print(\"\\nRMSE summary:\", {k: f\"{v:.4f}\" for k, v in rmse_scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55a7a4",
   "metadata": {},
   "source": [
    "<!-- # Model Training Notebook\n",
    "\n",
    "This notebook is dedicated to training machine learning models for the Yango Accra Mobility Prediction Hackathon. The goal is to predict ride times using trip and weather data.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Import required libraries\n",
    "2. Load and preprocess data\n",
    "3. Train-test split\n",
    "4. Train baseline models\n",
    "5. Train advanced models (e.g., LightGBM, XGBoost)\n",
    "6. Evaluate models using RMSE\n",
    "7. Save the best model -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset shape: (57596, 21)\n",
      "         trip_id  destination_lat  destination_lon lcl_start_transporting_dt  \\\n",
      "0  ID_S3BD1V9G53         5.630927        -0.169211                2024-05-05   \n",
      "1  ID_ZJM7LMN65Q         5.645044        -0.156482                2024-05-21   \n",
      "2  ID_SZ3BP6V01V         5.711156        -0.141063                2024-05-05   \n",
      "3  ID_5IPHXDCMKF         5.677497        -0.183350                2024-05-26   \n",
      "4  ID_BYZEJ0B5RA         5.601700        -0.173589                2024-05-30   \n",
      "\n",
      "  lcl_start_transporting_dttm  origin_lat  origin_lon  str_distance_km  \\\n",
      "0         2024-05-05 09:56:32    5.630979   -0.164760            0.529   \n",
      "1         2024-05-21 10:53:32    5.686892   -0.118931            6.230   \n",
      "2         2024-05-05 21:21:21    5.706008   -0.164999            2.705   \n",
      "3         2024-05-26 21:23:33    5.665943   -0.182602            1.236   \n",
      "4         2024-05-30 14:02:13    5.565401   -0.160919            4.312   \n",
      "\n",
      "   transporting_distance_fact_km  Target  ...  day_of_week  day_of_month  \\\n",
      "0                          0.850    2.18  ...            6             5   \n",
      "1                          8.720   20.93  ...            1            21   \n",
      "2                          3.239   13.02  ...            6             5   \n",
      "3                          1.410    3.80  ...            6            26   \n",
      "4                          6.553   17.23  ...            3            30   \n",
      "\n",
      "   is_weekend  is_rush_hour  haversine_distance_km            trip_hour  \\\n",
      "0           1             1               0.492566  2024-05-05 09:00:00   \n",
      "1           0             0               6.238359  2024-05-21 10:00:00   \n",
      "2           1             0               2.709589  2024-05-05 21:00:00   \n",
      "3           1             0               1.287398  2024-05-26 21:00:00   \n",
      "4           0             0               4.272852  2024-05-30 14:00:00   \n",
      "\n",
      "          weather_hour precipitation_type prev_hour_precipitation_mm  \\\n",
      "0  2024-05-05 09:00:00               Rain                   0.012875   \n",
      "1  2024-05-21 10:00:00               Rain                   0.001907   \n",
      "2  2024-05-05 21:00:00               Rain                   0.001907   \n",
      "3  2024-05-26 21:00:00   No precipitation                   0.000000   \n",
      "4  2024-05-30 14:00:00   No precipitation                   0.019073   \n",
      "\n",
      "   temperature_C  \n",
      "0          28.74  \n",
      "1          29.32  \n",
      "2          29.54  \n",
      "3          28.49  \n",
      "4          29.38  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "best_model = min(rmse_scores, key=rmse_scores.get)\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best RMSE: {rmse_scores[best_model]:.4f}\")\n",
    "\n",
    "# Save the best model - use the correct model objects\n",
    "best_model_obj = None\n",
    "if best_model == \"Random Forest\":\n",
    "    best_model_obj = rf_pipe  # Use the pipeline, not just rf_model\n",
    "elif best_model == \"Linear Regression\": \n",
    "    best_model_obj = lr_pipe  # Use the pipeline, not just lr_model\n",
    "elif best_model == \"LightGBM (CPU)\":\n",
    "    best_model_obj = lgb_model\n",
    "elif best_model == \"XGBoost (CPU)\":\n",
    "    best_model_obj = xgb_model\n",
    "\n",
    "print(f\"Selected model object: {type(best_model_obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed!\n",
      "Training set size: 46071 samples\n",
      "Test set size: 11518 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate submission file using the best model\n",
    "# Prepare test data\n",
    "X_test = test_df.drop(columns=['trip_id'])\n",
    "\n",
    "# Make predictions - handle both pipeline and direct models\n",
    "if best_model in [\"Random Forest\", \"Linear Regression\"]:\n",
    "    # For pipeline models (RF and LR), use raw data\n",
    "    test_predictions = best_model_obj.predict(X_test)\n",
    "else:\n",
    "    # For direct models (LightGBM and XGBoost), use encoded data\n",
    "    X_test_enc = preprocessor.transform(X_test)\n",
    "    if best_model == \"LightGBM (CPU)\":\n",
    "        test_predictions = best_model_obj.predict(X_test_enc, num_iteration=lgb_model.best_iteration)\n",
    "    else:  # XGBoost\n",
    "        test_predictions = best_model_obj.predict(X_test_enc)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'trip_id': test_df['trip_id'],\n",
    "    'travel_time': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = config.PROJECT_ROOT / \"outputs\" / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
